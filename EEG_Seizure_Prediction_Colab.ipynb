{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3d1cdd9",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc5917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Enable memory growth to avoid OOM\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f\"‚úì GPU configured: {gpus[0].name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda41a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q mne scipy scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c09abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (replace with your GitHub URL)\n",
    "# Option A: Clone from GitHub\n",
    "# !git clone https://github.com/YOUR_USERNAME/eeg-seizure-prediction.git\n",
    "# %cd eeg-seizure-prediction\n",
    "\n",
    "# Option B: Upload files manually\n",
    "# Use the file browser on the left to upload your project files\n",
    "\n",
    "# Option C: Mount Google Drive (if files are stored there)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Uncomment and modify the path if your project is in Google Drive:\n",
    "# %cd /content/drive/MyDrive/YourProjectFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93ba9f0",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Project Files Setup\n",
    "\n",
    "Run this cell to create all necessary project files directly in Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c6e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create project structure\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('training', exist_ok=True)\n",
    "os.makedirs('utils', exist_ok=True)\n",
    "\n",
    "print(\"‚úì Directory structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.py\n",
    "\"\"\"\n",
    "Configuration Settings for EEG Seizure Prediction\n",
    "\"\"\"\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__)) if '__file__' in dir() else os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"raw\")\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"saved_models\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
    "\n",
    "for dir_path in [DATA_DIR, PROCESSED_DIR, MODEL_DIR, RESULTS_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "@dataclass\n",
    "class EEGConfig:\n",
    "    original_sampling_rate: int = 256\n",
    "    target_sampling_rate: int = 256\n",
    "    n_channels: int = 22\n",
    "    bandpass_low: float = 0.5\n",
    "    bandpass_high: float = 40.0\n",
    "    notch_freq: float = 60.0\n",
    "    notch_width: float = 2.0\n",
    "    window_duration: float = 10.0\n",
    "    window_overlap: float = 0.5\n",
    "    preictal_duration: int = 300\n",
    "    seizure_prediction_horizon: int = 30\n",
    "\n",
    "    @property\n",
    "    def window_samples(self) -> int:\n",
    "        return int(self.window_duration * self.target_sampling_rate)\n",
    "\n",
    "@dataclass\n",
    "class CNNConfig:\n",
    "    conv_filters: Tuple[int, ...] = (64, 128, 256)\n",
    "    conv_kernel_sizes: Tuple[int, ...] = (7, 5, 3)\n",
    "    pool_sizes: Tuple[int, ...] = (2, 2, 2)\n",
    "    activation: str = \"relu\"\n",
    "    dropout_rate: float = 0.3\n",
    "    use_batch_norm: bool = True\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    d_model: int = 256\n",
    "    n_heads: int = 8\n",
    "    n_layers: int = 4\n",
    "    d_ff: int = 512\n",
    "    dropout_rate: float = 0.1\n",
    "    attention_dropout: float = 0.1\n",
    "    max_seq_length: int = 500\n",
    "\n",
    "@dataclass\n",
    "class ClassifierConfig:\n",
    "    hidden_units: Tuple[int, ...] = (128, 64)\n",
    "    dropout_rate: float = 0.5\n",
    "    n_classes: int = 1\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    train_ratio: float = 0.70\n",
    "    val_ratio: float = 0.15\n",
    "    test_ratio: float = 0.15\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 100\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    optimizer: str = \"adam\"\n",
    "    early_stopping_patience: int = 15\n",
    "    use_class_weights: bool = True\n",
    "    random_seed: int = 42\n",
    "\n",
    "eeg_config = EEGConfig()\n",
    "cnn_config = CNNConfig()\n",
    "transformer_config = TransformerConfig()\n",
    "classifier_config = ClassifierConfig()\n",
    "training_config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/cnn_encoder.py\n",
    "\"\"\"CNN Encoder for EEG feature extraction\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class CNNEncoder(layers.Layer):\n",
    "    def __init__(self, filters=(64, 128, 256), kernel_sizes=(7, 5, 3),\n",
    "                 pool_sizes=(2, 2, 2), dropout_rate=0.3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv_blocks = []\n",
    "        for i, (f, k, p) in enumerate(zip(filters, kernel_sizes, pool_sizes)):\n",
    "            self.conv_blocks.append([\n",
    "                layers.Conv1D(f, k, padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.MaxPooling1D(p, padding='same')\n",
    "            ])\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = tf.transpose(inputs, perm=[0, 2, 1])  # (batch, time, channels)\n",
    "        for conv, bn, pool in self.conv_blocks:\n",
    "            x = conv(x)\n",
    "            x = bn(x, training=training)\n",
    "            x = pool(x)\n",
    "        return self.dropout(x, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac176c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/transformer_encoder.py\n",
    "\"\"\"Transformer Encoder for temporal modeling\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_len=500, d_model=256, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        pe = np.zeros((max_len, d_model))\n",
    "        position = np.arange(0, max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        self.pe = tf.constant(pe[np.newaxis, :, :], dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pe[:, :tf.shape(x)[1], :]\n",
    "\n",
    "class TransformerEncoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model=256, n_heads=8, d_ff=512, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mha = layers.MultiHeadAttention(n_heads, d_model // n_heads, dropout=dropout)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(d_ff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.ln1 = layers.LayerNormalization()\n",
    "        self.ln2 = layers.LayerNormalization()\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        attn = self.mha(x, x, training=training)\n",
    "        x = self.ln1(x + self.dropout1(attn, training=training))\n",
    "        ffn_out = self.ffn(x)\n",
    "        return self.ln2(x + self.dropout2(ffn_out, training=training))\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, n_layers=4, d_model=256, n_heads=8, d_ff=512, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pos_encoding = PositionalEncoding(d_model=d_model)\n",
    "        self.enc_layers = [TransformerEncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "                          for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f3150",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/seizure_predictor.py\n",
    "\"\"\"Main CNN + Transformer Seizure Prediction Model\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from models.cnn_encoder import CNNEncoder\n",
    "from models.transformer_encoder import TransformerEncoder\n",
    "\n",
    "class ClassificationHead(layers.Layer):\n",
    "    def __init__(self, hidden_units=(128, 64), dropout_rate=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.global_pool = layers.GlobalAveragePooling1D()\n",
    "        self.dense_layers = [layers.Dense(u, activation='relu') for u in hidden_units]\n",
    "        self.dropout_layers = [layers.Dropout(dropout_rate) for _ in hidden_units]\n",
    "        self.output_layer = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        x = self.global_pool(x)\n",
    "        for dense, dropout in zip(self.dense_layers, self.dropout_layers):\n",
    "            x = dense(x)\n",
    "            x = dropout(x, training=training)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class SeizurePredictorCNNTransformer(Model):\n",
    "    def __init__(self, n_channels=22, n_timesteps=2560,\n",
    "                 conv_filters=(64, 128, 256), conv_kernel_sizes=(7, 5, 3),\n",
    "                 pool_sizes=(2, 2, 2), cnn_dropout=0.3,\n",
    "                 n_heads=8, n_transformer_layers=4, d_ff=512,\n",
    "                 transformer_dropout=0.1, classifier_hidden=(128, 64),\n",
    "                 classifier_dropout=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_channels = n_channels\n",
    "        self.n_timesteps = n_timesteps\n",
    "\n",
    "        self.cnn_encoder = CNNEncoder(conv_filters, conv_kernel_sizes,\n",
    "                                      pool_sizes, cnn_dropout)\n",
    "        d_model = conv_filters[-1]\n",
    "        self.transformer = TransformerEncoder(n_transformer_layers, d_model,\n",
    "                                              n_heads, d_ff, transformer_dropout)\n",
    "        self.classifier = ClassificationHead(classifier_hidden, classifier_dropout)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.cnn_encoder(inputs, training=training)\n",
    "        x = self.transformer(x, training=training)\n",
    "        return self.classifier(x, training=training)\n",
    "\n",
    "    def build_model(self):\n",
    "        dummy = tf.zeros((1, self.n_channels, self.n_timesteps))\n",
    "        _ = self.call(dummy)\n",
    "        return self\n",
    "\n",
    "def create_model(n_channels=22, n_timesteps=2560):\n",
    "    model = SeizurePredictorCNNTransformer(n_channels=n_channels, n_timesteps=n_timesteps)\n",
    "    model.build_model()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5925adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/__init__.py\n",
    "from models.seizure_predictor import SeizurePredictorCNNTransformer, create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64446928",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/preprocessing.py\n",
    "\"\"\"EEG Signal Preprocessing\"\"\"\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "class EEGPreprocessor:\n",
    "    def __init__(self, sampling_rate=256, bandpass_low=0.5, bandpass_high=40.0,\n",
    "                 notch_freq=60.0, window_duration=10.0, window_overlap=0.5):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.bandpass_low = bandpass_low\n",
    "        self.bandpass_high = bandpass_high\n",
    "        self.notch_freq = notch_freq\n",
    "        self.window_duration = window_duration\n",
    "        self.window_overlap = window_overlap\n",
    "        self._init_filters()\n",
    "\n",
    "    def _init_filters(self):\n",
    "        nyquist = self.sampling_rate / 2\n",
    "        low = max(0.001, min(self.bandpass_low / nyquist, 0.99))\n",
    "        high = max(0.001, min(self.bandpass_high / nyquist, 0.99))\n",
    "        self.bandpass_b, self.bandpass_a = signal.butter(4, [low, high], btype='band')\n",
    "        notch_norm = self.notch_freq / nyquist\n",
    "        if 0 < notch_norm < 1:\n",
    "            self.notch_b, self.notch_a = signal.iirnotch(notch_norm, self.notch_freq / 2)\n",
    "        else:\n",
    "            self.notch_b, self.notch_a = None, None\n",
    "\n",
    "    def bandpass_filter(self, data):\n",
    "        return signal.filtfilt(self.bandpass_b, self.bandpass_a, data, axis=-1)\n",
    "\n",
    "    def notch_filter(self, data):\n",
    "        if self.notch_b is not None:\n",
    "            return signal.filtfilt(self.notch_b, self.notch_a, data, axis=-1)\n",
    "        return data\n",
    "\n",
    "    def normalize(self, data):\n",
    "        mean = np.mean(data, axis=-1, keepdims=True)\n",
    "        std = np.std(data, axis=-1, keepdims=True) + 1e-8\n",
    "        return (data - mean) / std\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        data = self.bandpass_filter(data)\n",
    "        data = self.notch_filter(data)\n",
    "        data = self.normalize(data)\n",
    "        return data\n",
    "\n",
    "    def segment(self, data):\n",
    "        window_samples = int(self.window_duration * self.sampling_rate)\n",
    "        stride = int(window_samples * (1 - self.window_overlap))\n",
    "        n_samples = data.shape[-1]\n",
    "        windows = []\n",
    "        for start in range(0, n_samples - window_samples + 1, stride):\n",
    "            windows.append(data[..., start:start + window_samples])\n",
    "        return np.array(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652affb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/dataset.py\n",
    "\"\"\"Dataset utilities\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_synthetic_dataset(n_samples=1000, n_channels=22, n_timesteps=2560,\n",
    "                             preictal_ratio=0.3, seed=42):\n",
    "    \"\"\"Create synthetic EEG data for testing\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    n_preictal = int(n_samples * preictal_ratio)\n",
    "    n_interictal = n_samples - n_preictal\n",
    "\n",
    "    # Interictal: low amplitude random noise\n",
    "    X_interictal = np.random.randn(n_interictal, n_channels, n_timesteps) * 0.5\n",
    "\n",
    "    # Preictal: higher amplitude with spikes\n",
    "    X_preictal = np.random.randn(n_preictal, n_channels, n_timesteps) * 1.5\n",
    "    for i in range(n_preictal):\n",
    "        n_spikes = np.random.randint(5, 15)\n",
    "        for _ in range(n_spikes):\n",
    "            pos = np.random.randint(0, n_timesteps - 50)\n",
    "            channel = np.random.randint(0, n_channels)\n",
    "            spike = np.exp(-np.linspace(0, 3, 50)) * np.random.uniform(3, 8)\n",
    "            X_preictal[i, channel, pos:pos+50] += spike\n",
    "\n",
    "    X = np.concatenate([X_interictal, X_preictal], axis=0).astype(np.float32)\n",
    "    y = np.concatenate([np.zeros(n_interictal), np.ones(n_preictal)]).astype(np.float32)\n",
    "\n",
    "    # Shuffle\n",
    "    idx = np.random.permutation(n_samples)\n",
    "    X, y = X[idx], y[idx]\n",
    "\n",
    "    # Split\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=seed)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fc9bd3",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Quick Demo with Synthetic Data\n",
    "\n",
    "Run this section to test the model without downloading the actual dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a96750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from config import eeg_config, training_config\n",
    "from data.dataset import create_synthetic_dataset\n",
    "from models.seizure_predictor import create_model\n",
    "\n",
    "# Create synthetic dataset\n",
    "print(\"Creating synthetic dataset...\")\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = create_synthetic_dataset(\n",
    "    n_samples=500,\n",
    "    n_channels=22,\n",
    "    n_timesteps=2560,\n",
    "    preictal_ratio=0.3\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Labels: {y_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, Labels: {y_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, Labels: {y_test.shape}\")\n",
    "print(f\"Class distribution - Preictal: {y_train.sum()}/{len(y_train)} ({y_train.mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(\"\\nBuilding CNN + Transformer model...\")\n",
    "model = create_model(n_channels=22, n_timesteps=2560)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf10ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights for imbalanced data\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(\"‚úì Model compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ea0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,  # Reduced for demo\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f2e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {results[0]:.4f}\")\n",
    "print(f\"Test Accuracy: {results[1]:.2%}\")\n",
    "print(f\"Test AUC: {results[2]:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_prob = model.predict(X_test, verbose=0).flatten()\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Interictal', 'Preictal']))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf7749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Train')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "# AUC\n",
    "axes[2].plot(history.history['auc'], label='Train')\n",
    "axes[2].plot(history.history['val_auc'], label='Validation')\n",
    "axes[2].set_title('AUC')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e945b",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Save / Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba187eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('saved_models/cnn_transformer_colab.keras')\n",
    "print(\"‚úì Model saved to saved_models/cnn_transformer_colab.keras\")\n",
    "\n",
    "# Download to local machine\n",
    "from google.colab import files\n",
    "files.download('saved_models/cnn_transformer_colab.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "from models.seizure_predictor import SeizurePredictorCNNTransformer, ClassificationHead\n",
    "from models.cnn_encoder import CNNEncoder\n",
    "from models.transformer_encoder import TransformerEncoder, PositionalEncoding, TransformerEncoderLayer\n",
    "\n",
    "custom_objects = {\n",
    "    'SeizurePredictorCNNTransformer': SeizurePredictorCNNTransformer,\n",
    "    'CNNEncoder': CNNEncoder,\n",
    "    'TransformerEncoder': TransformerEncoder,\n",
    "    'ClassificationHead': ClassificationHead,\n",
    "    'PositionalEncoding': PositionalEncoding,\n",
    "    'TransformerEncoderLayer': TransformerEncoderLayer\n",
    "}\n",
    "\n",
    "# Uncomment to load a pretrained model:\n",
    "# loaded_model = tf.keras.models.load_model('saved_models/cnn_transformer_colab.keras',\n",
    "#                                           custom_objects=custom_objects)\n",
    "# print(\"‚úì Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b328e",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Run Inference on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seizure(model, eeg_data, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict seizure probability from EEG data.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        eeg_data: EEG array of shape (n_channels, n_samples) or (batch, n_channels, n_samples)\n",
    "        threshold: Classification threshold\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    # Add batch dimension if needed\n",
    "    if eeg_data.ndim == 2:\n",
    "        eeg_data = eeg_data[np.newaxis, ...]\n",
    "\n",
    "    # Predict\n",
    "    probabilities = model.predict(eeg_data, verbose=0).flatten()\n",
    "\n",
    "    results = {\n",
    "        'probabilities': probabilities,\n",
    "        'predictions': ['Preictal' if p >= threshold else 'Interictal' for p in probabilities],\n",
    "        'mean_probability': float(probabilities.mean()),\n",
    "        'max_probability': float(probabilities.max()),\n",
    "        'seizure_warning': probabilities.max() >= threshold\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example inference\n",
    "print(\"Running inference on test samples...\\n\")\n",
    "sample_data = X_test[:5]\n",
    "results = predict_seizure(model, sample_data)\n",
    "\n",
    "for i, (pred, prob) in enumerate(zip(results['predictions'], results['probabilities'])):\n",
    "    actual = 'Preictal' if y_test[i] == 1 else 'Interictal'\n",
    "    status = '‚úì' if pred == actual else '‚úó'\n",
    "    print(f\"Sample {i+1}: {pred} ({prob:.2%}) - Actual: {actual} {status}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Seizure Warning: {results['seizure_warning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835dff05",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Download Real CHB-MIT Dataset (Optional)\n",
    "\n",
    "Run this section to download actual EEG data from PhysioNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/download.py\n",
    "\"\"\"CHB-MIT Dataset Download\"\"\"\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "PHYSIONET_BASE_URL = \"https://physionet.org/files/chbmit/1.0.0\"\n",
    "\n",
    "@dataclass\n",
    "class SeizureInfo:\n",
    "    file_name: str\n",
    "    start_time: int\n",
    "    end_time: int\n",
    "\n",
    "@dataclass\n",
    "class PatientInfo:\n",
    "    patient_id: str\n",
    "    files: List[str]\n",
    "    seizures: List[SeizureInfo]\n",
    "\n",
    "def download_file(url, destination, verbose=True):\n",
    "    try:\n",
    "        if os.path.exists(destination):\n",
    "            if verbose: print(f\"  Exists: {os.path.basename(destination)}\")\n",
    "            return True\n",
    "        os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "        if verbose: print(f\"  Downloading: {os.path.basename(destination)}...\")\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        if verbose: print(f\"  ‚úì Downloaded: {os.path.basename(destination)}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def parse_seizure_summary(summary_path):\n",
    "    seizures, files = [], []\n",
    "    with open(summary_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    for block in re.split(r'File Name:', content)[1:]:\n",
    "        lines = block.strip().split('\\n')\n",
    "        file_name = lines[0].strip()\n",
    "        files.append(file_name)\n",
    "        if 'Number of Seizures in File: 0' not in block:\n",
    "            starts = re.findall(r'Seizure.*Start Time:\\s*(\\d+)', block)\n",
    "            ends = re.findall(r'Seizure.*End Time:\\s*(\\d+)', block)\n",
    "            for s, e in zip(starts, ends):\n",
    "                seizures.append(SeizureInfo(file_name, int(s), int(e)))\n",
    "    return seizures, files\n",
    "\n",
    "def download_chb_mit_sample(patient_ids=None, data_dir='data/raw', max_files=5, verbose=True):\n",
    "    if patient_ids is None:\n",
    "        patient_ids = ['chb01', 'chb02', 'chb03']\n",
    "    patients_info = {}\n",
    "    for pid in patient_ids:\n",
    "        if verbose: print(f\"\\nDownloading {pid}...\")\n",
    "        pdir = os.path.join(data_dir, pid)\n",
    "        os.makedirs(pdir, exist_ok=True)\n",
    "        summary_url = f\"{PHYSIONET_BASE_URL}/{pid}/{pid}-summary.txt\"\n",
    "        summary_path = os.path.join(pdir, f\"{pid}-summary.txt\")\n",
    "        if not download_file(summary_url, summary_path, verbose):\n",
    "            continue\n",
    "        seizures, file_list = parse_seizure_summary(summary_path)\n",
    "        files_with_sz = set(s.file_name for s in seizures)\n",
    "        priority = [f for f in file_list if f in files_with_sz]\n",
    "        other = [f for f in file_list if f not in files_with_sz]\n",
    "        to_download = (priority + other)[:max_files]\n",
    "        downloaded, sz_downloaded = [], []\n",
    "        for fname in to_download:\n",
    "            url = f\"{PHYSIONET_BASE_URL}/{pid}/{fname}\"\n",
    "            path = os.path.join(pdir, fname)\n",
    "            if download_file(url, path, verbose):\n",
    "                downloaded.append(fname)\n",
    "                sz_downloaded.extend([s for s in seizures if s.file_name == fname])\n",
    "        patients_info[pid] = PatientInfo(pid, downloaded, sz_downloaded)\n",
    "        if verbose: print(f\"  {len(downloaded)} files, {len(sz_downloaded)} seizures\")\n",
    "    return patients_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6783867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CHB-MIT data (uncomment to run)\n",
    "from data.download import download_chb_mit_sample\n",
    "\n",
    "# Download 3 patients with 5 files each (~500MB)\n",
    "# patients = download_chb_mit_sample(\n",
    "#     patient_ids=['chb01', 'chb02', 'chb03'],\n",
    "#     max_files=5,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "print(\"Uncomment the code above to download real EEG data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43613086",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "1. **Quick Demo** - Test with synthetic data (no downloads needed)\n",
    "2. **Model Training** - Train CNN+Transformer on GPU\n",
    "3. **Save/Load** - Export and import trained models\n",
    "4. **Inference** - Make predictions on new EEG data\n",
    "5. **Real Data** - Download CHB-MIT dataset from PhysioNet\n",
    "\n",
    "### Next Steps:\n",
    "- Upload your pretrained `.keras` files to load them\n",
    "- Download more patients for better model performance\n",
    "- Adjust hyperparameters in the config section\n",
    "\n",
    "---\n",
    "üß† **EEG Seizure Prediction System** | CNN + Transformer Architecture"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
